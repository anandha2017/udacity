{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an image classification model using very little data  \n",
    "\n",
    "Based on the tutorial by Francois Chollet @fchollet https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html and the workbook by Guillaume Dominici https://github.com/gggdominici/keras-workshop\n",
    "\n",
    "This tutorial presents several ways to build an image classifier using keras from just a few hundred or thousand pictures from each class you want to be able to recognize.\n",
    "\n",
    "We will go over the following options:  \n",
    "\n",
    "- training a small network from scratch (as a baseline)  \n",
    "- using the bottleneck features of a pre-trained network  \n",
    "- fine-tuning the top layers of a pre-trained network  \n",
    "  \n",
    "This will lead us to cover the following Keras features:   \n",
    "  \n",
    "- fit_generator for training Keras a model using Python data generators  \n",
    "- ImageDataGenerator for real-time data augmentation  \n",
    "- layer freezing and model fine-tuning  \n",
    "- ...and more.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
    "All you need is the train set  \n",
    "The recommended folder structure is:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data/\n",
    "    train/\n",
    "        dogs/ ### 1024 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 1024 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/ ### 416 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 416 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "Note : for this example we only consider 2x1000 training images and 2x400 testing images among the 2x12500 available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The github repo includes about 1500 images for this model. The original Kaggle dataset is much larger. The purpose of this demo is to show how you can build models with smaller size datasets. You should be able to improve this model by using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\r\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "#!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Conv Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape=(img_width, img_height,3)))\n",
    "#model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "#model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "#model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 2048\n",
    "nb_validation_samples = 832"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes\n",
    "\n",
    "The methods fit_generator, evaluate_generator and predict_generator now work by drawing a number of batches from a generator (number of training steps), rather than a number of samples.\n",
    "- samples_per_epoch was renamed steps_per_epoch in fit_generator.\n",
    "- nb_val_samples was renamed validation_steps in fit_generator.\n",
    "- val_samples was renamed steps in evaluate_generator and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=128, epochs=30, validation_steps=832)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 225s - loss: 0.7594 - acc: 0.5063 - val_loss: 0.6779 - val_acc: 0.6647\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 227s - loss: 0.6774 - acc: 0.5938 - val_loss: 0.6206 - val_acc: 0.6838\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 225s - loss: 0.6110 - acc: 0.6660 - val_loss: 0.5794 - val_acc: 0.7079\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 224s - loss: 0.5657 - acc: 0.7080 - val_loss: 0.5663 - val_acc: 0.6932\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 223s - loss: 0.5278 - acc: 0.7500 - val_loss: 0.5542 - val_acc: 0.7094\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 225s - loss: 0.4792 - acc: 0.7622 - val_loss: 0.5414 - val_acc: 0.7243\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 220s - loss: 0.4314 - acc: 0.7993 - val_loss: 0.5875 - val_acc: 0.7079\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 214s - loss: 0.3890 - acc: 0.8276 - val_loss: 0.6301 - val_acc: 0.7363\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 219s - loss: 0.3368 - acc: 0.8394 - val_loss: 0.6083 - val_acc: 0.7070\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 218s - loss: 0.3029 - acc: 0.8633 - val_loss: 0.6258 - val_acc: 0.7503\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 215s - loss: 0.2719 - acc: 0.8853 - val_loss: 0.6840 - val_acc: 0.7427\n",
      "Epoch 12/30\n",
      "128/128 [==============================] - 213s - loss: 0.2329 - acc: 0.9077 - val_loss: 0.7393 - val_acc: 0.7184\n",
      "Epoch 13/30\n",
      "128/128 [==============================] - 224s - loss: 0.1950 - acc: 0.9199 - val_loss: 0.9375 - val_acc: 0.7272\n",
      "Epoch 14/30\n",
      "128/128 [==============================] - 215s - loss: 0.1785 - acc: 0.9287 - val_loss: 0.8803 - val_acc: 0.7117\n",
      "Epoch 15/30\n",
      "128/128 [==============================] - 214s - loss: 0.1586 - acc: 0.9434 - val_loss: 0.9821 - val_acc: 0.7242\n",
      "Epoch 16/30\n",
      "128/128 [==============================] - 217s - loss: 0.1487 - acc: 0.9458 - val_loss: 1.2379 - val_acc: 0.7078\n",
      "Epoch 17/30\n",
      "128/128 [==============================] - 213s - loss: 0.1350 - acc: 0.9473 - val_loss: 1.3389 - val_acc: 0.7102\n",
      "Epoch 18/30\n",
      "128/128 [==============================] - 214s - loss: 0.1288 - acc: 0.9531 - val_loss: 1.2554 - val_acc: 0.7152\n",
      "Epoch 19/30\n",
      "128/128 [==============================] - 214s - loss: 0.1107 - acc: 0.9644 - val_loss: 1.4506 - val_acc: 0.7345\n",
      "Epoch 20/30\n",
      "128/128 [==============================] - 213s - loss: 0.1305 - acc: 0.9556 - val_loss: 1.8262 - val_acc: 0.7241\n",
      "Epoch 21/30\n",
      "128/128 [==============================] - 215s - loss: 0.1099 - acc: 0.9624 - val_loss: 1.2620 - val_acc: 0.6966\n",
      "Epoch 22/30\n",
      "128/128 [==============================] - 214s - loss: 0.0973 - acc: 0.9712 - val_loss: 1.5487 - val_acc: 0.7091\n",
      "Epoch 23/30\n",
      "128/128 [==============================] - 215s - loss: 0.1015 - acc: 0.9683 - val_loss: 1.8705 - val_acc: 0.7175\n",
      "Epoch 24/30\n",
      "128/128 [==============================] - 214s - loss: 0.1582 - acc: 0.9521 - val_loss: 2.6086 - val_acc: 0.6996\n",
      "Epoch 25/30\n",
      "128/128 [==============================] - 215s - loss: 0.0975 - acc: 0.9668 - val_loss: 1.9866 - val_acc: 0.7332\n",
      "Epoch 26/30\n",
      "128/128 [==============================] - 220s - loss: 0.1310 - acc: 0.9634 - val_loss: 1.6611 - val_acc: 0.7212\n",
      "Epoch 27/30\n",
      "128/128 [==============================] - 214s - loss: 0.1281 - acc: 0.9619 - val_loss: 1.9843 - val_acc: 0.7221\n",
      "Epoch 28/30\n",
      "128/128 [==============================] - 213s - loss: 0.1326 - acc: 0.9570 - val_loss: 1.8194 - val_acc: 0.7139\n",
      "Epoch 29/30\n",
      "128/128 [==============================] - 214s - loss: 0.0875 - acc: 0.9722 - val_loss: 1.4714 - val_acc: 0.7287\n",
      "Epoch 30/30\n",
      "128/128 [==============================] - 214s - loss: 0.1298 - acc: 0.9648 - val_loss: 1.8176 - val_acc: 0.7066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d4f8122b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('models_trained/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model successfully runs at one epoch, go back and it for 30 epochs by changing nb_epoch above.  I was able to get to an val_acc of 0.71 at 30 epochs.\n",
    "A copy of a pretrained network is available in the pretrained folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8119604979964117, 0.70695612980769229]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_no_dataaugmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After ~10 epochs the neural network reach ~70% accuracy. We can witness overfitting, no progress is made over validation set in the next epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation for improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.  \n",
    "This will hopefully reduce overfitting and allows better generalization capability for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of data augmentation applied to a picture:\n",
    "![Example of data augmentation applied to a picture](pictures/cat_data_augmentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "        rescale=1./255,        # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,       # randomly applies shearing transformation\n",
    "        zoom_range=0.2,        # randomly applies shearing transformation\n",
    "        horizontal_flip=True)  # randomly flip the images\n",
    "\n",
    "# same code as before\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=64, epochs=30, validation_steps=832)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "64/64 [==============================] - 223s - loss: 0.7402 - acc: 0.4893 - val_loss: 0.7037 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "64/64 [==============================] - 223s - loss: 0.6979 - acc: 0.5562 - val_loss: 0.6404 - val_acc: 0.6537\n",
      "Epoch 3/30\n",
      "64/64 [==============================] - 217s - loss: 0.6553 - acc: 0.6284 - val_loss: 0.6134 - val_acc: 0.6622\n",
      "Epoch 4/30\n",
      "64/64 [==============================] - 216s - loss: 0.6316 - acc: 0.6567 - val_loss: 0.5952 - val_acc: 0.6670\n",
      "Epoch 5/30\n",
      "64/64 [==============================] - 226s - loss: 0.6010 - acc: 0.6836 - val_loss: 0.5674 - val_acc: 0.7023\n",
      "Epoch 6/30\n",
      "64/64 [==============================] - 216s - loss: 0.5728 - acc: 0.7163 - val_loss: 0.5563 - val_acc: 0.6995\n",
      "Epoch 7/30\n",
      "64/64 [==============================] - 214s - loss: 0.5706 - acc: 0.7065 - val_loss: 0.5613 - val_acc: 0.7082\n",
      "Epoch 8/30\n",
      "64/64 [==============================] - 215s - loss: 0.5460 - acc: 0.7241 - val_loss: 0.5535 - val_acc: 0.6980\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - 215s - loss: 0.5528 - acc: 0.7334 - val_loss: 0.5998 - val_acc: 0.6798\n",
      "Epoch 10/30\n",
      "64/64 [==============================] - 215s - loss: 0.5402 - acc: 0.7412 - val_loss: 0.5203 - val_acc: 0.7370\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - 215s - loss: 0.5292 - acc: 0.7280 - val_loss: 0.5103 - val_acc: 0.7465\n",
      "Epoch 12/30\n",
      "64/64 [==============================] - 215s - loss: 0.5077 - acc: 0.7612 - val_loss: 0.4883 - val_acc: 0.7560\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - 215s - loss: 0.5237 - acc: 0.7510 - val_loss: 0.5166 - val_acc: 0.7402\n",
      "Epoch 14/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.7644"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0adc65d6a700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         nb_val_samples=nb_validation_samples)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1922\u001b[0m                                 \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m                                 pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1925\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2021\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator_augmented,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('models_trained/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69096329235113585, 0.76802884615384615]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 100 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_with_dataaugmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks to data-augmentation, the accuracy on the validation set improved to ~80%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
    "\n",
    "We can go beyond the previous models in terms of performance and efficiency by using a general-purpose, pre-trained image classifier.  This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
    "\n",
    "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 + small MLP\n",
    "![VGG16 + Dense layers Schema](pictures/vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading VGG16 weights\n",
    "This part is a bit complicated because the structure of our model is not exactly the same as the one used when training weights.  \n",
    "Otherwise, we would use the `model.load_weights()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : the VGG16 weights file (~500MB) is not included in this repository. You can download from here :  \n",
    "https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VGG16 model to process samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_bottleneck = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "validation_generator_bottleneck = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long process, so we save the output of the VGG16 once and for all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, nb_train_samples)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model_vgg.predict_generator(validation_generator_bottleneck, nb_validation_samples)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define and train the custom fully connected neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 832 samples\n",
      "Epoch 1/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.9784 - acc: 0.6855 - val_loss: 0.4140 - val_acc: 0.8077\n",
      "Epoch 2/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.4831 - acc: 0.7891 - val_loss: 0.3646 - val_acc: 0.8365\n",
      "Epoch 3/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3974 - acc: 0.8271 - val_loss: 0.3369 - val_acc: 0.8558\n",
      "Epoch 4/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3438 - acc: 0.8525 - val_loss: 0.3136 - val_acc: 0.8594\n",
      "Epoch 5/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3131 - acc: 0.8647 - val_loss: 0.5777 - val_acc: 0.7873\n",
      "Epoch 6/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2883 - acc: 0.8823 - val_loss: 0.2913 - val_acc: 0.8786\n",
      "Epoch 7/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2321 - acc: 0.9048 - val_loss: 0.3724 - val_acc: 0.8462\n",
      "Epoch 8/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2057 - acc: 0.9092 - val_loss: 0.4276 - val_acc: 0.8438\n",
      "Epoch 9/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2000 - acc: 0.9141 - val_loss: 0.3741 - val_acc: 0.8618\n",
      "Epoch 10/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1811 - acc: 0.9248 - val_loss: 0.3447 - val_acc: 0.8726\n",
      "Epoch 11/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1540 - acc: 0.9395 - val_loss: 0.3352 - val_acc: 0.8714\n",
      "Epoch 12/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1511 - acc: 0.9414 - val_loss: 0.3706 - val_acc: 0.8738\n",
      "Epoch 13/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1362 - acc: 0.9399 - val_loss: 0.3913 - val_acc: 0.8798\n",
      "Epoch 14/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1134 - acc: 0.9497 - val_loss: 0.3854 - val_acc: 0.8726\n",
      "Epoch 15/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1117 - acc: 0.9609 - val_loss: 0.5750 - val_acc: 0.8534\n",
      "Epoch 16/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0998 - acc: 0.9565 - val_loss: 0.4339 - val_acc: 0.8870\n",
      "Epoch 17/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0678 - acc: 0.9736 - val_loss: 0.4881 - val_acc: 0.8762\n",
      "Epoch 18/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0852 - acc: 0.9658 - val_loss: 0.4335 - val_acc: 0.8870\n",
      "Epoch 19/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0733 - acc: 0.9692 - val_loss: 0.4505 - val_acc: 0.8750\n",
      "Epoch 20/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0616 - acc: 0.9756 - val_loss: 0.5825 - val_acc: 0.8606\n",
      "Epoch 21/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0585 - acc: 0.9771 - val_loss: 0.6856 - val_acc: 0.8510\n",
      "Epoch 22/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0494 - acc: 0.9800 - val_loss: 0.5624 - val_acc: 0.8750\n",
      "Epoch 23/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0464 - acc: 0.9839 - val_loss: 0.8560 - val_acc: 0.8401\n",
      "Epoch 24/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0626 - acc: 0.9727 - val_loss: 0.5666 - val_acc: 0.8738\n",
      "Epoch 25/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0285 - acc: 0.9912 - val_loss: 0.5543 - val_acc: 0.8786\n",
      "Epoch 26/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0463 - acc: 0.9805 - val_loss: 0.6137 - val_acc: 0.8738\n",
      "Epoch 27/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0260 - acc: 0.9917 - val_loss: 0.6608 - val_acc: 0.8882\n",
      "Epoch 28/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0414 - acc: 0.9824 - val_loss: 0.6423 - val_acc: 0.8702\n",
      "Epoch 29/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0308 - acc: 0.9878 - val_loss: 0.6941 - val_acc: 0.8726\n",
      "Epoch 30/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.6677 - val_acc: 0.8678\n",
      "Epoch 31/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0315 - acc: 0.9927 - val_loss: 0.6830 - val_acc: 0.8738\n",
      "Epoch 32/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0258 - acc: 0.9922 - val_loss: 0.7593 - val_acc: 0.8798\n",
      "Epoch 33/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0266 - acc: 0.9922 - val_loss: 0.8203 - val_acc: 0.8546\n",
      "Epoch 34/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0198 - acc: 0.9941 - val_loss: 0.9712 - val_acc: 0.8450\n",
      "Epoch 35/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0291 - acc: 0.9902 - val_loss: 0.8274 - val_acc: 0.8594\n",
      "Epoch 36/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0168 - acc: 0.9941 - val_loss: 0.8353 - val_acc: 0.8738\n",
      "Epoch 37/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0324 - acc: 0.9912 - val_loss: 0.7481 - val_acc: 0.8702\n",
      "Epoch 38/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0191 - acc: 0.9932 - val_loss: 0.8011 - val_acc: 0.8750\n",
      "Epoch 39/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0254 - acc: 0.9897 - val_loss: 0.7558 - val_acc: 0.8702\n",
      "Epoch 40/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0267 - acc: 0.9897 - val_loss: 0.7828 - val_acc: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0b60feb90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch=40\n",
    "model_top.fit(train_data, train_labels,\n",
    "          nb_epoch=nb_epoch, batch_size=32,\n",
    "          validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process of this small neural network is very fast : ~2s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_top.save_weights('models/bottleneck_40_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_top.load_weights('models/with-bottleneck/1000-samples--100-epochs.h5')\n",
    "#model_top.load_weights('/notebook/Data1/Code/keras-workshop/models/with-bottleneck/1000-samples--100-epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/832 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.782802758165277, 0.87980769230769229]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_with_bottleneck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We reached a 90% accuracy on the validation after ~1m of training (~20 epochs) and 8% of the samples originally available on the Kaggle competition !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Fine-tuning the top layers of a a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start by instantiating the VGG base and loading its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier model to put on top of the convolutional model. For the fine tuning, we start with a fully trained-classifer. We will use the weights from the earlier model. And then we will add this model on top of the convolutional base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_model.load_weights('models/bottleneck_40_epochs.h5')\n",
    "\n",
    "model_vgg.add(top_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine turning, we only want to train a few layers.  This line will set the first 25 layers (up to the conv block) to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model_vgg.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model_vgg.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration  . . . do we need this?\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2048/2048 [==============================] - 39s - loss: 0.3593 - acc: 0.8862 - val_loss: 0.3669 - val_acc: 0.8738\n",
      "Epoch 2/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.2559 - acc: 0.9077 - val_loss: 0.3483 - val_acc: 0.8594\n",
      "Epoch 3/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1942 - acc: 0.9292 - val_loss: 0.3380 - val_acc: 0.8618\n",
      "Epoch 4/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1900 - acc: 0.9258 - val_loss: 0.3268 - val_acc: 0.8858\n",
      "Epoch 5/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1638 - acc: 0.9395 - val_loss: 0.3094 - val_acc: 0.8894\n",
      "Epoch 6/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1307 - acc: 0.9502 - val_loss: 0.3038 - val_acc: 0.8930\n",
      "Epoch 7/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1223 - acc: 0.9556 - val_loss: 0.3267 - val_acc: 0.8990\n",
      "Epoch 8/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1233 - acc: 0.9570 - val_loss: 0.2864 - val_acc: 0.8930\n",
      "Epoch 9/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1147 - acc: 0.9648 - val_loss: 0.3657 - val_acc: 0.8990\n",
      "Epoch 10/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1070 - acc: 0.9590 - val_loss: 0.3192 - val_acc: 0.8990\n",
      "Epoch 11/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0825 - acc: 0.9683 - val_loss: 0.4118 - val_acc: 0.8990\n",
      "Epoch 12/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0953 - acc: 0.9663 - val_loss: 0.3561 - val_acc: 0.9002\n",
      "Epoch 13/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0605 - acc: 0.9756 - val_loss: 0.3643 - val_acc: 0.9026\n",
      "Epoch 14/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0826 - acc: 0.9736 - val_loss: 0.3549 - val_acc: 0.8990\n",
      "Epoch 15/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0758 - acc: 0.9707 - val_loss: 0.3911 - val_acc: 0.9014\n",
      "Epoch 16/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0547 - acc: 0.9756 - val_loss: 0.4181 - val_acc: 0.9002\n",
      "Epoch 17/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0573 - acc: 0.9795 - val_loss: 0.3912 - val_acc: 0.9038\n",
      "Epoch 18/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0499 - acc: 0.9810 - val_loss: 0.4097 - val_acc: 0.9075\n",
      "Epoch 19/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0504 - acc: 0.9790 - val_loss: 0.4063 - val_acc: 0.9159\n",
      "Epoch 20/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0398 - acc: 0.9858 - val_loss: 0.4282 - val_acc: 0.9099\n",
      "Epoch 21/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0477 - acc: 0.9824 - val_loss: 0.4364 - val_acc: 0.8930\n",
      "Epoch 22/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0401 - acc: 0.9873 - val_loss: 0.4338 - val_acc: 0.9038\n",
      "Epoch 23/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0357 - acc: 0.9897 - val_loss: 0.4830 - val_acc: 0.9026\n",
      "Epoch 24/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0397 - acc: 0.9878 - val_loss: 0.4229 - val_acc: 0.9111\n",
      "Epoch 25/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0353 - acc: 0.9863 - val_loss: 0.4715 - val_acc: 0.9171\n",
      "Epoch 26/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0308 - acc: 0.9888 - val_loss: 0.4751 - val_acc: 0.9159\n",
      "Epoch 27/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0432 - acc: 0.9834 - val_loss: 0.4557 - val_acc: 0.9099\n",
      "Epoch 28/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0385 - acc: 0.9868 - val_loss: 0.4859 - val_acc: 0.9038\n",
      "Epoch 29/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0270 - acc: 0.9893 - val_loss: 0.4334 - val_acc: 0.9062\n",
      "Epoch 30/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0356 - acc: 0.9897 - val_loss: 0.4468 - val_acc: 0.9038\n",
      "Epoch 31/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0344 - acc: 0.9888 - val_loss: 0.4405 - val_acc: 0.9087\n",
      "Epoch 32/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.5071 - val_acc: 0.9050\n",
      "Epoch 33/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0179 - acc: 0.9937 - val_loss: 0.5163 - val_acc: 0.9111\n",
      "Epoch 34/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0397 - acc: 0.9868 - val_loss: 0.4168 - val_acc: 0.9123\n",
      "Epoch 35/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0240 - acc: 0.9941 - val_loss: 0.4653 - val_acc: 0.9111\n",
      "Epoch 36/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0206 - acc: 0.9932 - val_loss: 0.5174 - val_acc: 0.9062\n",
      "Epoch 37/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0308 - acc: 0.9902 - val_loss: 0.5016 - val_acc: 0.9087\n",
      "Epoch 38/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0159 - acc: 0.9966 - val_loss: 0.4876 - val_acc: 0.9111\n",
      "Epoch 39/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0335 - acc: 0.9883 - val_loss: 0.4280 - val_acc: 0.9050\n",
      "Epoch 40/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0198 - acc: 0.9922 - val_loss: 0.4693 - val_acc: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb059495c50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model_vgg.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.save_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.load_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46926221093879295, 0.90384615384615385]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69096328031558252, 0.76802884615384615]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/832 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.782802758165277, 0.87980769230769229]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data, validation_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
